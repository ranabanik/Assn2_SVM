{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6362-01 Advanced Machine Learning(2019F)\n",
    "**Assignment 2: Support Vector Classification** <br> \n",
    "by Rana Banik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: make_moon data from sklearn <br>\n",
    "In this SVC Gaussian RBF kernel[1] will be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$$ Kernel_{rbf} = e^{(-γ||x_1 - x_2||^2)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/moons_data.png\" style=\"width:500px;height:400px\">\n",
    "<img src = \"../Images/moons_data_3D.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: The data is from `sklearn`'s `datasets` class and called `make_moons`function. The above figures are 2000 samples of the data with 10% noise[2]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/noisy_data2.png\" style=\"width:800px;height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, two separate data sets will be generated as `train_set` and `test_set` each having 200 and 2000 data points with 40% noise. The two sets are shown in scatter plots above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A soft-margin SVM with an RBF kernel requires two parameters, C and γ. To find the best fit, at least five values per order-of-magnitude (i.e., per factor-of-ten) were taken in equal log space for both C and γ. For each combination of\n",
    "values of C and γ, fit an SVM to the training data and evaluate its test error. <br>\n",
    "best, `C0` = 1.0 <br>\n",
    "best, `γ0` = 1.0 <br>\n",
    "best test error = 13.3% <br>\n",
    "The decision boundary with lowest test error is shown below in contour plot:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/lowest_test_error_df2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of γ is fixed at γ = `γ0` and 100 different values of `log(C)` equally spaced between `log(C0)-3` to `log(C0)+3`. For each value of C the SVM models were fitted with `train_set`. The distribution of C is given in normal scale and logarithmic scale. The values of C are evenly distributed in the log scale. \n",
    "<img src = \"../Images/C_even_spaced.png\"> \n",
    "Two separate curves were generated based on the train error and test error, which is given below: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/traintestError_vs_logC2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** From the curves above it can be seen that the model generalizes well in test set when C = 1.0, which is the optimum tuning of the model. Before that both train and test error keeps decreasing. After log(C) threshold the model starts to overfit and starts to model the noises in the train set. Thus train error decreases significantly and test error keeps increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for different C: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phenomena can also be seen in the following graphs. As the value of C increases the decision boundary becomes more complex and deforms to nonlinear pattern to classify maximum points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 contour plots of the decision function for different values of C. _The\n",
    "plots should highlight the decision boundaries, but not the margins_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/c_vary_df2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With increasing of parameter C means the model starts giving importance to individual data points, which in terms increases the influence of each datapoint on the decision boundary. Thus as the C increases and reaches optimum value training error is high but test error is lowest since the model is generalized. But after that the model starts to overfit and becomes more complex which increases test error and decreases train error. A small C means a very restricted model where each data point can only have very limited influence. Increasing C means these data points can have a stronger influence and makes the decision boundary bend to classify them[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/traintestError_vs_gamma2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** The train and test error tends to decrease as γ increases. The test error becomes lowest in γ = 1.0 where the model is optimized for the data being used. After that overfitting on the training set starts and train error keeps reducing. Contrarily, the model loses it's generalization and test error increases largely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for different γ: <br>\n",
    "The similar effect of increasing C is also inspected in increasing γ. If γ increases the radius of Gaussian RBF kernel reduces exponentially. That in turns creates clustered decision boundary rather that linear/non-linear one. Thus if γ increases the model complexity increases. So the model is more generalized at γ0. After that testing error increases and since model starts to overfit the training error decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../Images/γ_vary_df2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 contour plots of the decision function for different values of γ are shown above. We respectively increase the value of γ from 0.0010 to 1000.0. A small γ means a large radius for the Gaussian kernel, which means many points are considered close by. This is reflected by very smooth decision boundaries in top plots. A low value of γ means the the decision boundary will vary slowly, which yields model of low complexity. On the other hand a high value of γ yields a more complex model[4]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning Data Mining, Inference, and Prediction (12th printing). Springer, 2017.<br>\n",
    " 2. “SUPPORT VECTOR MACHINES(SVM) - Towards Data Science.”\n",
    " 3. Introduction to data mining, P.-N. Tan, M. Steinbach, A. Karpatne, and V. Kumar. \n",
    " 4. “Introduction to Machine Learning with Python: A Guide for Data Scientists - Andreas C. Müller, Muller Andreas C, Sarah Guido - Google Books.”\n",
    " 5. “RBF SVM parameters — scikit-learn 0.21.3 documentation.” [Online]. Available: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0e-03, 2.8e-03, 4.6e-03, 6.4e-03, 8.2e-03, 1.0e-02, 2.8e-02,\n",
       "       4.6e-02, 6.4e-02, 8.2e-02, 1.0e-01, 2.8e-01, 4.6e-01, 6.4e-01,\n",
       "       8.2e-01, 1.0e+00, 2.8e+00, 4.6e+00, 6.4e+00, 8.2e+00, 1.0e+01,\n",
       "       2.8e+01, 4.6e+01, 6.4e+01, 8.2e+01, 1.0e+02, 2.8e+02, 4.6e+02,\n",
       "       6.4e+02, 8.2e+02, 1.0e+03, 2.8e+03, 4.6e+03, 6.4e+03, 8.2e+03])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(-3,4):\n",
    "    c_list = np.append(c_list, np.linspace(10**i, 10**(i+1), num=5, endpoint=False))\n",
    "c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-03, 1.14975700e-03, 1.32194115e-03, 1.51991108e-03,\n",
       "       1.74752840e-03, 2.00923300e-03, 2.31012970e-03, 2.65608778e-03,\n",
       "       3.05385551e-03, 3.51119173e-03, 4.03701726e-03, 4.64158883e-03,\n",
       "       5.33669923e-03, 6.13590727e-03, 7.05480231e-03, 8.11130831e-03,\n",
       "       9.32603347e-03, 1.07226722e-02, 1.23284674e-02, 1.41747416e-02,\n",
       "       1.62975083e-02, 1.87381742e-02, 2.15443469e-02, 2.47707636e-02,\n",
       "       2.84803587e-02, 3.27454916e-02, 3.76493581e-02, 4.32876128e-02,\n",
       "       4.97702356e-02, 5.72236766e-02, 6.57933225e-02, 7.56463328e-02,\n",
       "       8.69749003e-02, 1.00000000e-01, 1.14975700e-01, 1.32194115e-01,\n",
       "       1.51991108e-01, 1.74752840e-01, 2.00923300e-01, 2.31012970e-01,\n",
       "       2.65608778e-01, 3.05385551e-01, 3.51119173e-01, 4.03701726e-01,\n",
       "       4.64158883e-01, 5.33669923e-01, 6.13590727e-01, 7.05480231e-01,\n",
       "       8.11130831e-01, 9.32603347e-01, 1.07226722e+00, 1.23284674e+00,\n",
       "       1.41747416e+00, 1.62975083e+00, 1.87381742e+00, 2.15443469e+00,\n",
       "       2.47707636e+00, 2.84803587e+00, 3.27454916e+00, 3.76493581e+00,\n",
       "       4.32876128e+00, 4.97702356e+00, 5.72236766e+00, 6.57933225e+00,\n",
       "       7.56463328e+00, 8.69749003e+00, 1.00000000e+01, 1.14975700e+01,\n",
       "       1.32194115e+01, 1.51991108e+01, 1.74752840e+01, 2.00923300e+01,\n",
       "       2.31012970e+01, 2.65608778e+01, 3.05385551e+01, 3.51119173e+01,\n",
       "       4.03701726e+01, 4.64158883e+01, 5.33669923e+01, 6.13590727e+01,\n",
       "       7.05480231e+01, 8.11130831e+01, 9.32603347e+01, 1.07226722e+02,\n",
       "       1.23284674e+02, 1.41747416e+02, 1.62975083e+02, 1.87381742e+02,\n",
       "       2.15443469e+02, 2.47707636e+02, 2.84803587e+02, 3.27454916e+02,\n",
       "       3.76493581e+02, 4.32876128e+02, 4.97702356e+02, 5.72236766e+02,\n",
       "       6.57933225e+02, 7.56463328e+02, 8.69749003e+02, 1.00000000e+03])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list = np.power(10, np.linspace(-3, 3, num=100))\n",
    "C0 = 1\n",
    "c_list_ = np.logspace(np.log10(C0)-3,np.log10(C0)+3,num=100,endpoint=True,base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-03, 1.14975700e-03, 1.32194115e-03, 1.51991108e-03,\n",
       "       1.74752840e-03, 2.00923300e-03, 2.31012970e-03, 2.65608778e-03,\n",
       "       3.05385551e-03, 3.51119173e-03, 4.03701726e-03, 4.64158883e-03,\n",
       "       5.33669923e-03, 6.13590727e-03, 7.05480231e-03, 8.11130831e-03,\n",
       "       9.32603347e-03, 1.07226722e-02, 1.23284674e-02, 1.41747416e-02,\n",
       "       1.62975083e-02, 1.87381742e-02, 2.15443469e-02, 2.47707636e-02,\n",
       "       2.84803587e-02, 3.27454916e-02, 3.76493581e-02, 4.32876128e-02,\n",
       "       4.97702356e-02, 5.72236766e-02, 6.57933225e-02, 7.56463328e-02,\n",
       "       8.69749003e-02, 1.00000000e-01, 1.14975700e-01, 1.32194115e-01,\n",
       "       1.51991108e-01, 1.74752840e-01, 2.00923300e-01, 2.31012970e-01,\n",
       "       2.65608778e-01, 3.05385551e-01, 3.51119173e-01, 4.03701726e-01,\n",
       "       4.64158883e-01, 5.33669923e-01, 6.13590727e-01, 7.05480231e-01,\n",
       "       8.11130831e-01, 9.32603347e-01, 1.07226722e+00, 1.23284674e+00,\n",
       "       1.41747416e+00, 1.62975083e+00, 1.87381742e+00, 2.15443469e+00,\n",
       "       2.47707636e+00, 2.84803587e+00, 3.27454916e+00, 3.76493581e+00,\n",
       "       4.32876128e+00, 4.97702356e+00, 5.72236766e+00, 6.57933225e+00,\n",
       "       7.56463328e+00, 8.69749003e+00, 1.00000000e+01, 1.14975700e+01,\n",
       "       1.32194115e+01, 1.51991108e+01, 1.74752840e+01, 2.00923300e+01,\n",
       "       2.31012970e+01, 2.65608778e+01, 3.05385551e+01, 3.51119173e+01,\n",
       "       4.03701726e+01, 4.64158883e+01, 5.33669923e+01, 6.13590727e+01,\n",
       "       7.05480231e+01, 8.11130831e+01, 9.32603347e+01, 1.07226722e+02,\n",
       "       1.23284674e+02, 1.41747416e+02, 1.62975083e+02, 1.87381742e+02,\n",
       "       2.15443469e+02, 2.47707636e+02, 2.84803587e+02, 3.27454916e+02,\n",
       "       3.76493581e+02, 4.32876128e+02, 4.97702356e+02, 5.72236766e+02,\n",
       "       6.57933225e+02, 7.56463328e+02, 8.69749003e+02, 1.00000000e+03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21147d8a438>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(c_list_)\n",
    "# plt.plot(np.log10(c_list_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matplotlib.inline` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from bonnerLib import dfContour, df3D\n",
    "# %matplotlib.inline\n",
    "\n",
    "data_path = r'C:\\Data\\CS6362\\Assignment2'\n",
    "\n",
    "\"\"\"%%% Load the data %%%\"\"\"\n",
    "data = datasets.make_moons(n_samples=2000, noise=0.1)\n",
    "X,y = data\n",
    "plt.figure()\n",
    "plt.suptitle('Moons data sample')\n",
    "colors = np.array(['r','b'])\n",
    "# plt.scatter(X[:,0],X[:,1],color = colors[y],s=3)\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"%%% Set Params, fit and RBF kernel %%%\"\"\"\n",
    "clf = SVC(gamma=1.0, C=1.0)\n",
    "clf.fit(X,y)\n",
    "\n",
    "\"\"\"%%% Plot figure 1 and 2\"\"\"\n",
    "dfContour(clf, data)\n",
    "df3D(clf, data)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"%%% Make predictions\"\"\"\n",
    "# clf.predict(Xtest)\n",
    "# clf.decision_function(Xtest)\n",
    "\n",
    "\n",
    "print('Taking train and test data...')\n",
    "train_set = datasets.make_moons(n_samples=200, noise=0.4,random_state=1)\n",
    "train_data, train_label = train_set\n",
    "test_set = datasets.make_moons(n_samples=2000, noise=0.4,random_state=0)\n",
    "test_data, test_label = test_set\n",
    "# plt.figure()\n",
    "# plt.subplot(1, 2, 1)\n",
    "# colors = np.array(['r','b'])\n",
    "# plt.scatter(train_data[:, 0], train_data[:, 1], color=colors[train_label], s=3)\n",
    "# plt.title(\"train data\",fontsize=15)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# colors = np.array(['r', 'b'])\n",
    "# plt.scatter(test_data[:, 0], test_data[:, 1], color=colors[test_label], s=3)\n",
    "# plt.title(\"test data\",fontsize=15)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"save...\")\n",
    "# save_path = os.path.join(data_path,'test_set.bin')\n",
    "# # print(save_path)\n",
    "# # with open(save_path,'wb') as pfile:\n",
    "# #     pickle.dump(test_set, pfile)\n",
    "\n",
    "# print(\"loading the same set of data...\")\n",
    "# save_path = os.path.join(data_path, 'test_set.bin')\n",
    "# with open(save_path,'rb') as pfile:\n",
    "#     test_set = pickle.load(pfile)\n",
    "\n",
    "# save_path = os.path.join(data_path, 'train_set.bin')\n",
    "# with open(save_path,'rb') as pfile:\n",
    "#     train_set = pickle.load(pfile)\n",
    "\n",
    "# train_data, train_label = train_set\n",
    "# test_data, test_label = test_set\n",
    "# plt.figure()\n",
    "# plt.subplot(1, 2, 1)\n",
    "# colors = np.array(['r','b'])\n",
    "# plt.scatter(train_data[:, 0], train_data[:, 1], color=colors[train_label], s=3)\n",
    "# plt.title(\"train data\")\n",
    "# plt.subplot(1, 2, 2)\n",
    "# colors = np.array(['r', 'b'])\n",
    "# plt.scatter(test_data[:, 0], test_data[:, 1], color=colors[test_label], s=3)\n",
    "# plt.title(\"test data\")\n",
    "# plt.show()\n",
    "\n",
    "# C = np.logspace(4.0, 4.01, num=5, endpoint=True, base=10)\n",
    "# # plt.plot(np.log10(C))\n",
    "# # plt.show()\n",
    "# print(np.log10(C))\n",
    "# print(C)\n",
    "\"\"\"%%%% Greedy search %%%%\"\"\"\n",
    "# start = -4.0\n",
    "# end = -\n",
    "# n = 0\n",
    "# points = []\n",
    "#\n",
    "# while n < 10:\n",
    "#     # C = np.logspace(start, end, num=5, endpoint=False, base=10)\n",
    "#     # points.append(C)\n",
    "#     print(start, end)\n",
    "#     start = end\n",
    "#     end += 1\n",
    "#     n += 1\n",
    "#\n",
    "#\n",
    "# print(np.array(points).shape)\n",
    "# points = np.array(points).ravel()\n",
    "\n",
    "points = np.array([])\n",
    "\n",
    "for i in range(-3,4):\n",
    "    points = np.append(points, np.linspace(10**i, 10**(i+1), num=5, endpoint=True)) #doesn't need to ravel()\n",
    "\n",
    "count = 0\n",
    "accuracy_best = 0\n",
    "\n",
    "for c in points:\n",
    "    for gamma in points:\n",
    "        clf = SVC(gamma=gamma, C=c)\n",
    "        clf.fit(train_data,train_label)\n",
    "        accuracy = clf.score(test_data,test_label)\n",
    "        count += 1\n",
    "        if accuracy>accuracy_best:\n",
    "            C0 = c\n",
    "            gamma0 = gamma\n",
    "            accuracy_best = accuracy\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(\"best fit:\", C0, gamma0, count, accuracy_best) #best fit: 1.0 1.0 1225 0.867\n",
    "#1000.0 0.09942600739529568 2500 0.87\n",
    "# plt.plot(np.log10(points))\n",
    "# plt.show()\n",
    "\n",
    "if __name__ != \"__main__\":\n",
    "    C0 = 1.0\n",
    "    gamma0 = 1.0\n",
    "    accuracy_best = 0.867\n",
    "\n",
    "\"\"\"Plot best model\"\"\"\n",
    "clf = SVC(gamma=gamma0, C=C0)\n",
    "clf.fit(train_data,train_label)\n",
    "dfContour(clf, test_set)\n",
    "plt.title(\"Decision boundary with lowest test error: {}%\".format((1-accuracy_best)*100))\n",
    "df3D(clf, test_set)\n",
    "# clf.decision_function(Xtest)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Error plot for fixed γ\"\"\"\n",
    "test_err = []\n",
    "train_err = []\n",
    "start = np.log10(C0) - 3 #todo find values here\n",
    "end = np.log10(C0) + 3 #todo\n",
    "# following C are both the same\n",
    "C = np.logspace(start, end, num=100, endpoint=True, base=10)\n",
    "# C = np.power(10, np.linspace(start, end, num=100))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(C,label= \"C\")\n",
    "plt.legend(loc='upper left', fontsize=15)\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(np.log10(C),label = \"log10(C)\")\n",
    "plt.legend(loc='upper left', fontsize=15)\n",
    "# plt.plot(C)\n",
    "plt.suptitle(\"C evenly spaced in log scale\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for c in C:\n",
    "    clf = SVC(gamma=gamma0,C=c)\n",
    "    clf.fit(train_data,train_label)\n",
    "    err = 1 - clf.score(train_data,train_label)\n",
    "    train_err.append(err)\n",
    "    err = 1 - clf.score(test_data, test_label)\n",
    "    test_err.append(err)\n",
    "\n",
    "plt.plot(np.log10(C),train_err,'b',linewidth=2,label=\"Train error\")\n",
    "plt.plot(np.log10(C),test_err,'g',linewidth=2,label='Test error')\n",
    "plt.axvline(np.log10(C0))\n",
    "plt.xlabel(\"log10(C)\",fontsize=15)\n",
    "plt.ylabel(\"Error(a.u.)\",fontsize=15)\n",
    "plt.legend(loc='upper right', fontsize=15)\n",
    "plt.title(\"Train and test error for γ: {:.4f}\".format(gamma0))\n",
    "plt.show()\n",
    "\n",
    "start = np.log10(C0) - 3\n",
    "end = np.log10(C0) + 3\n",
    "# C = np.logspace(start,end,num=7,endpoint=True,base=10)\n",
    "C = np.power(10, np.linspace(start, end, num=7)) #these two lines get same results\n",
    "\n",
    "for i,c in enumerate(C):\n",
    "    clf = SVC(gamma=gamma0,C=c)\n",
    "    clf.fit(train_data,train_label)\n",
    "    plt.subplot(4,2,i+1)\n",
    "    dfContour(clf, train_set)\n",
    "    plt.title(\"C = {}\".format(c))\n",
    "plt.suptitle(\"Decision function with varied C\")\n",
    "plt.tight_layout(pad=0.1, w_pad=0.5, h_pad=0.7)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Error plot for fixed C\"\"\"\n",
    "test_err = []\n",
    "train_err = []\n",
    "start = np.log10(gamma0) - 3\n",
    "end = np.log10(gamma0) + 3\n",
    "gamma = np.logspace(start, end, num=100, endpoint=True, base=10)\n",
    "for g in gamma:\n",
    "    clf = SVC(gamma=g,C=C0)\n",
    "    clf.fit(train_data,train_label)\n",
    "    err = 1 - clf.score(train_data,train_label)\n",
    "    train_err.append(err)\n",
    "    err = 1 - clf.score(test_data, test_label)\n",
    "    test_err.append(err)\n",
    "plt.plot(np.log10(gamma),train_err,'b',linewidth=2,label=\"Train error\")\n",
    "plt.plot(np.log10(gamma),test_err,'g',linewidth=2,label='Test error')\n",
    "plt.axvline(np.log10(gamma0))\n",
    "plt.xlabel(\"log10(γ)\",fontsize=15)\n",
    "plt.ylabel(\"Error(a.u.)\",fontsize=15)\n",
    "plt.legend(loc='upper left', fontsize=15)\n",
    "plt.title(\"Train and test error for C: {:.2f}\".format(C0))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "start = np.log10(gamma0) - 3\n",
    "end = np.log10(gamma0) + 3\n",
    "gamma = np.logspace(start,end,num=7,endpoint=True,base=10)\n",
    "\n",
    "for i,g in enumerate(gamma):\n",
    "    clf = SVC(gamma=g,C=C0)\n",
    "    clf.fit(train_data,train_label)\n",
    "    plt.subplot(4,2,i+1)\n",
    "    dfContour(clf, train_set)\n",
    "    plt.title(\"γ = {:.4f}\".format(g))\n",
    "plt.suptitle(\"Decision function with varied γ\")\n",
    "plt.tight_layout(pad=0.1, w_pad=0.5, h_pad=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jan 28 13:35:55 2017\n",
    "\n",
    "@author: anthonybonner\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# plot the data points in red and blue (for 0 and 1, respectively).\n",
    "# then plot the contours of the decision function (of classifier clf)\n",
    "# and highlight the decision boundary in solid black.\n",
    "# If margins=1 then highlight the margins in dashed black\n",
    "\n",
    "def dfContour(clf, data, margins=0):\n",
    "    X, y = data\n",
    "    # plot the data\n",
    "    colors = np.array(['r', 'b'])\n",
    "    plt.scatter(X[:, 0], X[:, 1], color=colors[y], s=3)\n",
    "\n",
    "    # form a mesh/grid to cover the data\n",
    "    h = 0.02\n",
    "    x_min = X[:, 0].min() - .5\n",
    "    x_max = X[:, 0].max() + .5\n",
    "    y_min = X[:, 1].min() - .5\n",
    "    y_max = X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    mesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # evaluate the decision functrion at the grid points\n",
    "    Z = clf.decision_function(mesh)\n",
    "\n",
    "    # plot the contours of the decision function\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, levels=[-4, -3, -2, -1, 0, 1, 2, 3, 4], cmap=cm.RdBu, alpha=0.5)\n",
    "\n",
    "    # draw the decision boundary in solid black\n",
    "    plt.contour(xx, yy, Z, levels=[0], colors='k', linestyles='solid')\n",
    "    if margins:\n",
    "        # draw the margins in dashed black\n",
    "        plt.contour(xx, yy, Z, levels=[-1, 1], colors='k', linestyles='dashed')\n",
    "\n",
    "\n",
    "# plot the decision function of classifier clf in 3D.\n",
    "# if Cflag=1, place a contour plot of the decision function beneath the 3D plot.\n",
    "# (Use data to determine the range of the axes)\n",
    "\n",
    "def df3D(clf, data, cFlag=1):\n",
    "    # form a mesh/grid to cover the data\n",
    "    h = 0.01\n",
    "    X, y = data\n",
    "    x_min = X[:, 0].min() - .5\n",
    "    x_max = X[:, 0].max() + .5\n",
    "    y_min = X[:, 1].min() - .5\n",
    "    y_max = X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    mesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # evaluate the decision functrion at the grid points\n",
    "    Z = clf.decision_function(mesh)\n",
    "    Z = -Z  # to improve the 3D plot for the Moons data set, negate Z\n",
    "\n",
    "    # plot the contours of the decision function\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(xx, yy, Z, cmap=cm.RdBu, linewidth=0.2, edgecolor='k')\n",
    "\n",
    "    if cFlag == 1:\n",
    "        # display a contour plot of the decision function\n",
    "        Zmin = np.min(Z) - 1.0\n",
    "        ax.contourf(xx, yy, Z, cmap=cm.RdBu, offset=Zmin)\n",
    "        ax.set_zlim(bottom = Zmin)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
